---
title: 利用 Kafka、Spark Streaming、Pyspark 整合搭建流式数据分析环境
date: 2019-05-12 14:16:06
tags: Spark
index_img: https://ws2.sinaimg.cn/large/006tNc79ly1g2yir9flp8j315p0u0aw5.jpg
banner_img: https://ws2.sinaimg.cn/large/006tNc79ly1g2yip3lqadj31900u0hdu.jpg
---

## 目标

主要需求是用 Scrapy-Redis 爬取的数据直接由 Scrapy Pipeline 接入 Kafka，再利用 Kafka 消息队列作为数据源，Python 操作 Spark Streaming 接收 Kafka 数据进行实时计算。

这样比定时任务或者 MapReduce 更加灵活。也可以省去 Scrapy 的数据存入 MongoDB 后，再从 MongoDB 取数据分析这样繁琐和消耗资源的操作。

<!--more-->

本篇主要是进行后一部分 ``data -> kafka -> Spark Streaming -> result`` 的组合配置的测试试验，使用 Python 版本为3.6.2。至于Spark Streaming 的数据流分析的一些策略(比如流式数据并不是时间序列, 数据的累积计算等)暂时没有很好的思路，欢迎有经验的同学一起交流一下。

经过无数踩坑，总结出一种可行的版本组合，大多数存在的问题也基本都是版本匹配和 jar 包版本问题，为了试验的简便，暂时没有做分布式的配置和测试。

**如有纰漏或错误，欢迎及时指出。**

## 基本环境及配置

### 实验环境

(版本非常重要，安装软件请严格按照以下要求，否则可能出现各种问题)

- CentOS 7.6
- Python-3.6.2
- JDK-1.8.0
- Scala-2.11
- Hadoop-2.7.7
- Spark-2.3.3
- Kafka-2.2.0

### 环境搭建

本篇着重于核心部分的配置说明，有关于JDK，Python 以及 Scala 和 Hadoop 的基本搭建可以自行按照制定的版本进行安装并配置好环境变量，保证能够正常使用后继续进行下一步的操作。否则不能确保最终搭建成功。

本篇所有软件包都直接下载在 ``/home/hadoop/`` 等同于 ``~/`` 目录下，所以下文配置的路径和有关命令也以此为准。请根据自己的实际情况作出更改。

#### Hadoop 和 JDK-1.8.0

这一部分内容较多，也是基础工作，不过多提及。可参照网上其他相关教程做好配置工作。

#### Python-3.6.2

安装配置好 Python-3.6.2。

简略步骤：

##### 0. 下载安装相关必要的工具

```shell
yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gcc make
```

我系统环境中自带 python2.7, 备份一下

```shell
mv /usr/bin/python /usr/bin/python.bak
```

##### 1. 官网下载 Python-3.6.2

```shell
wget https://www.python.org/ftp/python/3.6.2/Python-3.6.2.tar.xz
```

##### 2. 解压-编译-安装

```shell
tar -zxvf  Python-3.6.2.tar.xz

cd Python-3.6.2

./configure prefix=/usr/local/python3

make && make install
```

##### 3. 配置环境

添加软链接。

```shell
ln -s /usr/local/python3/bin/python3 /usr/bin/python
```

pip 也添加一下。

```shell
ln -s /usr/local/python3/bin/pip3 /usr/bin/pip
```

因为执行yum需要python2版本，所以我们还要修改yum的配置，执行：

```shell
vi /usr/bin/yum
```

把 ``#! /usr/bin/python`` 修改为 ``#! /usr/bin/python2``

同理:

```shell
vi /usr/libexec/urlgrabber-ext-down
```

文件里面的 ``#! /usr/bin/python`` 也要修改为 ``#! /usr/bin/python2``

##### 4. 最终结果

使用 ``python2`` 命令可以对应 ``python2.7``

使用 ``python`` 命令可以对应 ``python3.6.2``

使用 ``pip list`` 命令能显示已安装的包。

##### 5. 还要安装一些所需的包

```shell
pip install kafka
```

```shell
pip install pyspark
```

#### Scala-2.11

Spark-2.3.3 自带 Scala-2.11

#### Kafka-2.2.0

**下载 Kafka-2.2.0**

```shell
cd ~

wget http://mirrors.tuna.tsinghua.edu.cn/apache/kafka/2.2.0/kafka-2.2.0-src.tgz
```

**解压**

```shell
tar -zxvf kafka-2.2.0-src.tgz

mv kafka-2.2.0-src.tgz kafka
```

**测试**

因为下载的 kafka 已经自带 Zookeeper，不需要额外安装，运行 Kafka 之前直接使用。

首先使用一个命令行窗口启动 Zookeeper.

```shell
cd ~/kafka

bin/zookeeper-server-start.sh config/zookeeper.properties
```

启动成功后不要关闭终端或退出，否则 Zookeeper 服务也会停止。打开一个新的终端，用来启动 Kafka.

```shell
cd ~/kafka

bin/kafka-server-start.sh config/server.properties
```

kafka 启动后同样不要关闭，再打开一个新的终端，进行测试。

创建一个 topic，topic 是消息发布的 category，接受消息也需要指定 topic

```shell
cd ~/kafka

bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic testmsg
```

创建一个 producer 来生产消息。

```shell
bin/kafka-console-producer.sh --broker-list localhost:9092 --topic testmsg
```

运行成功后 出现 > 输入几条数据如:

``hello world``

``hello kafka``

``hello spark``

再次开启一个新的终端，创建一个 consumer 来接收消息。

```shell
cd ~/kafka

bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic testmag --from-beginning
```

如果一切正常，出现上面在 producer 输入的消息。说明 kafka 能够正常使用。

#### Spark-2.3.3 环境搭建及测试

##### 1. 安装配置 Spark-2.3.3

Saprk 2.3.3 适用于 Scala 2.11版本，所以务必保证已经安装 Scala 2.11。这是 Spark 网站上的说明：
> Note that, Spark is pre-built with Scala 2.12 since version 2.4.2. Previous versions are pre-built with Scala 2.11.

其实因为已经安装有 Hadoop2.7.7 环境， 可以选择不带Hadoop 的 ``spark-2.3.3-bin-without-hadoop.tgz`` 下载，但是这个版本没有国内镜像链接，我们下载另一个也是有效的。

使用清华大学开源镜像站的链接：

```shell
http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.3.3/spark-2.3.3-bin-hadoop2.7.tgz
```

**下载-解压-改名**

```shell
cd ~

wget http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.3.3/spark-2.3.3-bin-hadoop2.7.tgz

tar -zxvf spark-2.3.3-bin-hadoop2.7.tgz

mv spark-2.3.3-bin-hadoop2.7.tgz spark
```

**配置 Spark**

进入 ``spark/conf/`` 目录，将 ``conf/`` 目录中的 ``spark-env.sh.template`` 拷贝一份为 ``spark-env.sh 作为 Spark`` 的配置文件：

```shell
cd spark/conf

mv spark-env.sh.template spark-env.sh

vi spark-env.sh
```

编辑，在文件第二行开始添加：

>注：/usr/bin/python 为你环境的 python 命令目录，spark默认使用 Python2，我们使用 Python3，且确保 python 命令能使用 python3，如果 python3 命令能运行则改成 python3

```shell
export SPARK_DIST_CLASSPATH=$(/home/hadoop/hadoop/bin/hadoop classpath):/home/hadoop/spark/jars/kafka/*:/home/hadoop/kafka/libs/*
export PYSPARK_PYTHON=/usr/bin/python
```

这样就指定了相关 jar 包的路径。

在 ``spark/libs`` 目录中下载相关 jar 包（**重要**）

在这一步需要严格按照 Kafka 和 Spark 的版本下载 jar 包，组合运行时很多错误就是在这一步造成的。

我们使用的版本 Scala-2.11, Kafka-2.2.0, Spark-2.3.3 所以需要在 mvnrepository.com 下载3个 jar 包。

(有些版本只说下载第二个，但在这个版本组合下我实际情况测试后也会出问题，提示无法找到相关方法或者无法连接 kafka consumer）

官方说明中 ``kafka-0-10`` 是不支持 Python 的，所以注意我们下载的版本为 ``kafka-0-8``：

``spark-streaming-kafka-0-8-assembly_2.11-2.3.3.jar``

``spark-streaming-kafka-0-8_2.11-2.3.3.jar``

``spark-core_2.11-2.3.3.jar``

```shell
cd ~/spark/libs

wget http://central.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.3.3/spark-streaming-kafka-0-8-assembly_2.11-2.3.3.jar

wget http://central.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.3.3/spark-streaming-kafka-0-8_2.11-2.3.3.jar

wget http://central.maven.org/maven2/org/apache/spark/spark-core_2.11/2.3.3/spark-core_2.11-2.3.3.jar
```

在 ``spark/jars`` 下新建 ``kafka`` 文件夹，将 ``kafka/libs`` 下的所有 jar 包都拷贝进去。

```shell
cp ~/kafka/libs* .
```

到这 spark 单节点的配置基本就完成了，下面测试一下。

##### 2. 测试 Spark

首先测试 pyspark。

在终端进入 ``spark/bin``, 输入 ``./spark-shell``

```shell
cd ~/spark/bin

./spark-shell
```

稍等一会，在一堆日志信息后，如果出现如下大大的 Spark 图案和 scala 命令行说明 Spark 启动成功。

```shell
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.3.3
      /_/

Using Scala version 2.11.8 (OpenJDK 64-Bit Server VM, Java 1.8.0_201)
Type in expressions to have them evaluated.
Type :help for more information.

scala>
```

退出后，再在当前目录输入 ``./pysaprk``

```shell
./pyspark
```

出现以下界面也就说明 pyspark 配置成功。再继续输入 ``import kafka`` 和 ``import pyspark`` 测试相关模块是否正确安装。
(如果不成功，检查 pyspark 和 kafka 模块是否安装)

```shell
Python 3.6.2 (default, May  2 2019, 04:58:04)
[GCC 4.8.5 20150623 (Red Hat 4.8.5-36)] on linux
Type "help", "copyright", "credits" or "license" for more information.
······
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
2019-05-07 03:15:36 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.3.3
      /_/

Using Python version 3.6.2 (default, May  2 2019 04:58:04)
SparkSession available as 'spark'.
>>>import kafka
>>>import pyspark
>>>
```

### Kafka 与 Spark Streaming 组合测试运行

#### 代码准备

在合适的位置下新建一个 ``producer.py`` 模拟 Kafka 生产数据。

``producer.py`` 代码如下：

```python
from kafka import KafkaProducer
import time
import csv

# 连接 Kafka
producer = KafkaProducer(bootstrap_servers='localhost:9092')
# user_log.csv 为自己虚拟的数据，建议可以使用单个字符或单词，以 ',' 隔开
# 我这里以随机的 1 和 0 为数据
csvfile = open("data/user_log.csv", "r")
reader = csv.reader(csvfile)

for line in reader:
    data = line[0]
    # 延时 0.1 秒
    time.sleep(0.1)
    print(data)
    producer.send('test', data.encode('utf8'))
```

接下来，新建一个 ``wordcount.py`` 文件，使用 kafka 作为数据源，来测试组合后能否正常消费 kafka 里的数据。

``wordcount.py`` 代码如下:

```python
from pyspark import SparkContext, SparkConf
from pyspark.streaming import StreamingContext
from pyspark.streaming.kafka import KafkaUtils

def start():
    conf = SparkConf().setAppName("TestCount")
    sc = SparkContext(conf=conf)
    sc.setLogLevel("ERROR")
    # 设置 3 秒计算一次
    ssc=StreamingContext(sc,3)
    # 使用 checkpoint，所以需要启动 Hadoop
    ssc.checkpoint(".")
    # 使用 createStream 方式创建流
    kafkaStreams = KafkaUtils.createStream(ssc,"localhost:2181","1",{"test":1})
    # 简单的计算一下， x[1] 为数据的位置
    result = kafkaStreams.map(lambda x: (x[1], 1)).reduceByKey(lambda a, b: a + b)
    result.pprint()
    ssc.start()
    ssc.awaitTermination()

if __name__ == '__main__':
    start()
```

#### 组合测试运行

现在默认你的服务器环境中所有服务都为未启动状态，且上文要求的环境也都完成配置

##### 1. 启动 Hadoop

使用 checkpoint 把数据存入 HDFS，所以需要启动 Hadoop

```shell
cd ~/hadoop/sbin/

./start-all.sh
```

##### 2. 启动 Zookeeper 服务和 Kafka

参照上文启动 Zookeeper 和 Kafka 的方法:

先启动 Zookeeper.

```shell
cd ~/kafka

bin/zookeeper-server-start.sh config/zookeeper.propreties
```

启动 Zookeeper 成功后用另一个终端启动 kafka.

```shell
cd ~/kafka

bin/kafka-server-start.sh config/zookeeper.propreties
```

如果有错误检查是否端口被占用或者之前开启的服务没停止，有的话直接 kill 掉再重新操作上述步骤。

##### 3. 运行 producer.py

再用一个新的终端运行 ``producer.py``

```shell
python producer.py
```

```shell
···
b'1'
b'0'
b'1'
b'1'
b'1'
b'0'
b'1'
b'1'
b'0'
b'1'
b'0'
···
```

运行成功会打印出数据，不要退出或关闭，再用新的终端执行下面的操作。

##### 4. 运行 wordcount.py

这里需要用到 ``spark-submit`` 命令将 ``wordcount.py`` 提交给 spark 运行。

```shell
spark-submit wordcount.py
```

在一大堆提示信息后出现如下，就说明运行成功。

```shell
spark-submit wordcount.py
······
2019-05-07 05:06:40 INFO  SparkContext:54 - Running Spark version 2.3.3
·······

-------------------------------------------
Time: 2019-05-07 05:15:30
-------------------------------------------
('1', 20)
('0', 10)

-------------------------------------------
Time: 2019-05-07 05:15:33
-------------------------------------------
('0', 11)
('1', 19)

-------------------------------------------
Time: 2019-05-07 05:15:36
-------------------------------------------
('1', 19)
('0', 10)

-------------------------------------------
Time: 2019-05-07 05:15:39
-------------------------------------------
('1', 19)
('0', 11)
```

结果显示的就是 Spark Streaming 按照预期按每三秒一次计算出了 1 和 0 出现的次数。

到这基本就说明我们 ``Kafka -> Spark Streaming`` 的测试环境搭建和组合配置都是成功的，有兴趣的同学可以继续尝试将 Kafka 和 Spark 都进行分布式的配置。